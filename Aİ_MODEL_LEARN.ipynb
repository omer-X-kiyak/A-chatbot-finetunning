{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWehldv4/xGntex+GMlngA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omer-X-kiyak/A-chatbot-finetunning/blob/main/A%C4%B0_MODEL_LEARN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9nAxch3lkxY"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade peft\n",
        "!pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "njCG_ImvltwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"\"\"###SYSTEM: Based on INPUT title generate the prompt for generative model\n",
        "\n",
        "###INPUT: Linux Terminal\n",
        "\n",
        "###PROMPT:\"\"\"\n",
        "tokens = tokenizer(txt, return_tensors=\"pt\")['input_ids'].to(\"cuda\")\n",
        "op = model.generate(tokens, max_new_tokens=200)\n",
        "print(tokenizer.decode(op[0]))"
      ],
      "metadata": {
        "id": "fuu_7z1LoD9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1, peft_type=TaskType.CAUSAL_LM)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "print(model.print_trainable_parameters())"
      ],
      "metadata": {
        "id": "6ypcebhqoVJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Awnn83dYohBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\", split=\"train\")\n",
        "print(dataset[0].keys())\n",
        "\n",
        "dataset = dataset.map(format_dataset)\n",
        "print(dataset[0].keys())"
      ],
      "metadata": {
        "id": "esREKAY9onF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dJ8foacSotic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.remove_columns(['act', \"prompt\"])\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "cJsfsPSYox_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = tmp[\"train\"]\n",
        "test_dataset = tmp[\"test\"]\n",
        "print(train_dataset)\n",
        "print(test_dataset)\n"
      ],
      "metadata": {
        "id": "52amba7Doz2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYtinJ0xo4HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ],
      "metadata": {
        "id": "KbyvN06lo4xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "                    model = model,\n",
        "                    train_dataset=train_dataset,\n",
        "                    eval_dataset = test_dataset,\n",
        "                    tokenizer = tokenizer,\n",
        "                    data_collator = data_collator,\n",
        "\n",
        "                    args = TrainingArguments(\n",
        "                        output_dir=\"./training\",\n",
        "                        remove_unused_columns=False,\n",
        "                        per_device_train_batch_size=2,\n",
        "                        gradient_checkpointing=True,\n",
        "                        gradient_accumulation_steps=4,\n",
        "                        max_steps=200,\n",
        "                        learning_rate=2.5e-5,\n",
        "                        logging_steps=5,\n",
        "                        fp16=True,\n",
        "                        optim=\"paged_adamw_8bit\",\n",
        "                        save_strategy=\"steps\",\n",
        "                        save_steps=50,\n",
        "                        evaluation_strategy=\"steps\",\n",
        "                        eval_steps=5,\n",
        "                        do_eval=True,\n",
        "                        label_names = [\"input_ids\", \"labels\", \"attention_mask\"],\n",
        "                        report_to = \"none\",\n",
        "\n",
        "                ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZXEX_ldpYN4",
        "outputId": "1df5124e-53a3-47d4-f0bd-0d89c240e1bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MmtFHDCQpfWs",
        "outputId": "4aae7534-96b0-42a5-bccb-58d7ef078b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 11:55, Epoch 11/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.062600</td>\n",
              "      <td>3.167049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.975500</td>\n",
              "      <td>3.133137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.994800</td>\n",
              "      <td>3.096320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.896000</td>\n",
              "      <td>3.075357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.872800</td>\n",
              "      <td>3.044265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.770500</td>\n",
              "      <td>3.004627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.923600</td>\n",
              "      <td>2.972040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.784400</td>\n",
              "      <td>2.932237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.771900</td>\n",
              "      <td>2.898241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.675000</td>\n",
              "      <td>2.855613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.663000</td>\n",
              "      <td>2.821333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.690100</td>\n",
              "      <td>2.788810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.628300</td>\n",
              "      <td>2.747082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.525000</td>\n",
              "      <td>2.708443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.580800</td>\n",
              "      <td>2.671035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.460400</td>\n",
              "      <td>2.626580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>2.430500</td>\n",
              "      <td>2.585062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.345400</td>\n",
              "      <td>2.539543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>2.356600</td>\n",
              "      <td>2.473738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.341500</td>\n",
              "      <td>2.424920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>2.171200</td>\n",
              "      <td>2.377701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.131100</td>\n",
              "      <td>2.333656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>2.228900</td>\n",
              "      <td>2.292488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.080600</td>\n",
              "      <td>2.243197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>2.053400</td>\n",
              "      <td>2.210420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.043400</td>\n",
              "      <td>2.168425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>1.985000</td>\n",
              "      <td>2.147665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.005800</td>\n",
              "      <td>2.119164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>1.936500</td>\n",
              "      <td>2.109251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.878400</td>\n",
              "      <td>2.091405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>1.935500</td>\n",
              "      <td>2.076192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.972300</td>\n",
              "      <td>2.068277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>1.890400</td>\n",
              "      <td>2.054659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.810900</td>\n",
              "      <td>2.040841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>1.832600</td>\n",
              "      <td>2.030775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.884500</td>\n",
              "      <td>2.030352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>1.856700</td>\n",
              "      <td>2.024039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.848200</td>\n",
              "      <td>2.025248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>1.866800</td>\n",
              "      <td>2.021124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.850500</td>\n",
              "      <td>2.016072</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=2.3252925157546995, metrics={'train_runtime': 720.8898, 'train_samples_per_second': 2.219, 'train_steps_per_second': 0.277, 'total_flos': 2527687722663936.0, 'train_loss': 2.3252925157546995, 'epoch': 11.594202898550725})"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating TextÂ¶\n"
      ],
      "metadata": {
        "id": "nDIeKCbPpxtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"\"\"###SYSTEM: Based on INPUT title generate the prompt for generative model\n",
        "\n",
        "###INPUT: Math Tutor\n",
        "\n",
        "###PROMPT:\"\"\"\n",
        "tokens = tokenizer(txt, return_tensors=\"pt\")['input_ids'].to(\"cuda\")\n",
        "op = model.generate(tokens, max_new_tokens=200)\n",
        "output = tokenizer.decode(op[0], skip_special_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "bGXx2G43pzNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving PEFT model loraÂ¶\n"
      ],
      "metadata": {
        "id": "OGzRAogbs4Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"prompt_250_steps\", safe_serialization=False, )\n"
      ],
      "metadata": {
        "id": "E7XW9A9Ns5j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r prompt_250.zip '/content/prompt_250_steps'\n"
      ],
      "metadata": {
        "id": "eAUHUy7YqcZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading PEFT model weights\n"
      ],
      "metadata": {
        "id": "3Y3WMxFitJlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Modelin doÄŸru dizinden yÃ¼klendiÄŸinden emin olun\n",
        "model_path = \"/content/prompt_250_steps\"\n",
        "\n",
        "# PeftModel'in Ã¶nceden eÄŸitilmiÅŸ model ile yÃ¼klenmesi\n",
        "model = PeftModel.from_pretrained(model, model_path)\n"
      ],
      "metadata": {
        "id": "UtKgTragtCD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ = model.merge_and_unload()\n",
        "model_.save_pretrained(\"merged_model_\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfN8xw6itRf9",
        "outputId": "680a1499-7989-4634-81f8-ad26feac6440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:83: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TLCKVyrtPiTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/programminghut/finetune-llm-hf"
      ],
      "metadata": {
        "id": "j-BdrrkZwf1X"
      }
    }
  ]
}